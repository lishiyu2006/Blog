# RAG
RAGï¼ˆRetrieval-Augmented Generationï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ˜¯ä¸€ç§ç»“åˆä¿¡æ¯æ£€ç´¢ä¸è¯­è¨€ç”Ÿæˆçš„æŠ€æœ¯ï¼Œå¸¸ç”¨äºæå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç‰¹å®šé¢†åŸŸæˆ–çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§ä¸ç›¸å…³æ€§ã€‚ä¸‹é¢æˆ‘å°†å¸¦ä½ ä¸€æ­¥æ­¥å®æ“ä¸€ä¸ªç®€å•çš„ RAG ç³»ç»Ÿã€‚
## ä¸€ã€RAG åŸºæœ¬åŸç†

RAG = **æ£€ç´¢å™¨ï¼ˆRetrieverï¼‰** + **ç”Ÿæˆå™¨ï¼ˆGeneratorï¼‰**

1. **ç”¨æˆ·æé—®**Â â†’
2. **æ£€ç´¢å™¨**Â ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼ˆå¦‚ PDFã€ç½‘é¡µã€æ•°æ®åº“ç­‰ï¼‰â†’
3. **å°†é—®é¢˜ + æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡**Â è¾“å…¥ç»™ LLM â†’
4. **LLM ç”Ÿæˆç­”æ¡ˆ**ï¼ˆåŸºäºæ£€ç´¢å†…å®¹ï¼Œé¿å…â€œå¹»è§‰â€ï¼‰

---

## äºŒã€å®æ“ç¯å¢ƒå‡†å¤‡

æˆ‘ä»¬å°†ä½¿ç”¨ Python + å¼€æºå·¥å…·æ­å»ºä¸€ä¸ªæœ¬åœ° RAG ç³»ç»Ÿã€‚

### æ‰€éœ€åº“ï¼š

```bash
pip install langchain
pip install faiss-cpu  # å‘é‡æ•°æ®åº“ï¼ˆä¹Ÿå¯ç”¨ chromaã€weaviate ç­‰ï¼‰
pip install sentence-transformers  # ç”¨äºåµŒå…¥ï¼ˆEmbeddingï¼‰
pip install transformers  # å¯é€‰ï¼Œç”¨äºæœ¬åœ° LLM
pip install pypdf  # å¦‚æœè¦è¯»å– PDF
```
> ğŸ’¡ ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ OpenAI API ä½œä¸º LLMï¼Œä½†è¿™é‡Œæˆ‘ä»¬å°½é‡ç”¨å¼€æºæ–¹æ¡ˆã€‚

---

## ä¸‰ã€å‡†å¤‡çŸ¥è¯†åº“ï¼ˆç¤ºä¾‹ï¼‰

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªæœ¬åœ°çŸ¥è¯†åº“ï¼š`data/` ç›®å½•ä¸‹æœ‰å‡ ä¸ª `.txt` æ–‡ä»¶ï¼Œå†…å®¹æ˜¯å…³äºâ€œäººå·¥æ™ºèƒ½å‘å±•å²â€çš„ç‰‡æ®µã€‚

ä¾‹å¦‚ï¼š`ai_history.txt`
```
äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰èµ·æºäº20ä¸–çºª50å¹´ä»£ã€‚1956å¹´è¾¾ç‰¹èŒ…æ–¯ä¼šè®®è¢«è®¤ä¸ºæ˜¯AIçš„è¯ç”Ÿæ ‡å¿—ã€‚
...
```

---

## å››ã€æ„å»º RAG ç³»ç»Ÿï¼ˆä»£ç ï¼‰

### æ­¥éª¤ 1ï¼šåŠ è½½æ–‡æ¡£
```python
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# åŠ è½½æ–‡æ¡£
loader = TextLoader("data/ai_history.txt", encoding="utf-8")
documents = loader.load()

# åˆ†å—ï¼ˆchunkï¼‰
text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
chunks = text_splitter.split_documents(documents)
```

### æ­¥éª¤ 2ï¼šåˆ›å»ºå‘é‡æ•°æ®åº“ï¼ˆä½¿ç”¨ FAISSï¼‰

```python
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

# ä½¿ç”¨å¼€æºåµŒå…¥æ¨¡å‹ï¼ˆå¦‚ all-MiniLM-L6-v2ï¼‰
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# æ„å»ºå‘é‡åº“
vectorstore = FAISS.from_documents(chunks, embeddings)
#chunk ä¼šè¢«å°è£…æˆ Document å¯¹è±¡ï¼Œå¹¶ä½œä¸ºå‘é‡åº“çš„åŸºæœ¬å­˜å‚¨å’Œæ£€ç´¢å•å…ƒã€‚
```
#### æ•°æ®åº“é€‰æ‹©ï¼šå·¥å…·åº“ vs å®Œæ•´ç³»ç»Ÿ

- **FAISS (Facebook AI Similarity Search):** å®ƒæ˜¯ç”± Meta å¼€å‘çš„ä¸€ä¸ª**é«˜æ€§èƒ½ç´¢å¼•åº“**ã€‚å®ƒä¸“æ³¨äºå¦‚ä½•åœ¨å†…å­˜ä¸­ä»¥æå¿«çš„é€Ÿåº¦è¿›è¡Œå‘é‡ç›¸ä¼¼åº¦æœç´¢ã€‚å®ƒä¸å…·å¤‡å­˜å‚¨åŸå§‹æ–‡æ¡£ã€å…ƒæ•°æ®è¿‡æ»¤ã€ç”¨æˆ·æƒé™ç®¡ç†æˆ–ç½‘ç»œæ¥å£ç­‰åŠŸèƒ½ã€‚
    
- **å‘é‡æ•°æ®åº“ (å¦‚ Milvus, Pinecone):** å®ƒä»¬åœ¨åº•å±‚é€šå¸¸é›†æˆäº† FAISS æˆ–ç±»ä¼¼çš„ç®—æ³•åº“ï¼Œä½†åœ¨å¤–å±‚åŒ…è£¹äº†æ•°æ®åº“çš„åŠŸèƒ½ï¼Œå¦‚ï¼š**æŒä¹…åŒ–å­˜å‚¨ã€CRUDï¼ˆå¢åˆ æ”¹æŸ¥ï¼‰ã€API æ¥å£ã€å¤šç§Ÿæˆ·éš”ç¦»ã€é«˜å¯ç”¨æ€§**ç­‰ã€‚
### å¸¸ç”¨æ¨¡å‹æ¨è

| ç”¨é€”       | æ¨¡å‹åç§°                                                          | ç‰¹ç‚¹                                   |
| -------- | ------------------------------------------------------------- | ------------------------------------ |
| **é€šç”¨ä¸­æ–‡** | `BAAI/bge-small-zh-v1.5`                                      | è½»é‡ã€é«˜æ•ˆã€ä¸­æ–‡æ•ˆæœå¥½                          |
| **å¤šè¯­è¨€**  | `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` | æ”¯æŒ50+è¯­è¨€                              |
| **é•¿æ–‡æœ¬**  | `jina-embeddings-v3`ï¼ˆè§çŸ¥è¯†åº“ [2]ï¼‰                                | æ”¯æŒÂ **8192 tokens**ï¼Œæ”¯æŒä»»åŠ¡å®šåˆ¶ï¼ˆTask LoRAï¼‰ |
| **è‹±æ–‡é¦–é€‰** | `all-MiniLM-L6-v2`                                            | å°å·§å¿«é€Ÿï¼Œè‹±æ–‡æ•ˆæœä½³                           |

> âœ… ä¸­æ–‡ RAG é¡¹ç›®å¼ºçƒˆæ¨è `BAAI/bge-*` ç³»åˆ—ï¼ˆç”±æ™ºæºç ”ç©¶é™¢å‘å¸ƒï¼‰ï¼Œåœ¨ MTEB ä¸­æ–‡æ¦œå•ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚

### æ­¥éª¤ 3ï¼šè®¾ç½®æ£€ç´¢å™¨
```python
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})  # è¿”å›æœ€ç›¸å…³çš„3ä¸ªç‰‡
```
> **â€œç‰‡â€å°±æ˜¯æŒ‡`Document` å¯¹è±¡**ï¼Œå®ƒåŒ…å«ï¼š
> 
>   - `.page_content`ï¼šåŸå§‹æ–‡æœ¬å†…å®¹ï¼ˆå³ä½ åˆ†å—åçš„ chunk æ–‡æœ¬ï¼‰
>   - `.metadata`ï¼šå…ƒæ•°æ®ï¼ˆå¦‚æ¥æºæ–‡ä»¶åã€é¡µç ã€åˆ†å—åºå·ç­‰ï¼‰
### æ­¥éª¤ 4ï¼šé€‰æ‹© LLMï¼ˆè¿™é‡Œç”¨ HuggingFace çš„æœ¬åœ°æ¨¡å‹ï¼‰
```python

from langchain.llms import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# ä½¿ç”¨è¾ƒå°çš„å¼€æºæ¨¡å‹ï¼ˆå¦‚ google/flan-t5-baseï¼‰
model_name = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

pipe = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer,
    max_length=512,
    temperature=0.0,
)

llm = HuggingFacePipeline(pipeline=pipe)
```
> âš ï¸ æ³¨æ„ï¼š`flan-t5-base` æ˜¯ encoder-decoder æ¨¡å‹ï¼Œé€‚åˆé—®ç­”ï¼›è‹¥ç”¨ LLaMA ç­‰ decoder-only æ¨¡å‹ï¼Œéœ€è°ƒæ•´ pipelineã€‚

### æ­¥éª¤ 5ï¼šæ„å»º RAG é“¾
```python
from langchain.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",  # å°†æ‰€æœ‰æ£€ç´¢ç»“æœæ‹¼æ¥åè¾“å…¥ LLM
    retriever=retriever,
    return_source_documents=True
)
```

### æ­¥éª¤ 6ï¼šæé—®å¹¶æŸ¥çœ‹ç»“æœ
```python
query = "äººå·¥æ™ºèƒ½æ˜¯åœ¨å“ªä¸€å¹´æ­£å¼æå‡ºçš„ï¼Ÿ"
result = qa_chain({"query": query})

print("å›ç­”:", result["result"])
print("å‚è€ƒæ¥æº:")
for doc in result["source_documents"]:
    print("-", doc.page_content[:100] + "...")
```

---

## äº”ã€è¿›é˜¶å»ºè®®

1. **ä½¿ç”¨æ›´å¼ºå¤§çš„ LLM**ï¼šå¦‚ LLaMA-2ã€ChatGLMã€Qwenï¼ˆéœ€ GPUï¼‰
2. **ä½¿ç”¨ Chroma æˆ– Weaviate**Â æ›¿ä»£ FAISSï¼Œæ”¯æŒæŒä¹…åŒ–
3. **åŠ å…¥ reranker**ï¼ˆå¦‚ Cohere Rerank æˆ– BAAI/bge-rerankerï¼‰æå‡æ£€ç´¢è´¨é‡
4. **éƒ¨ç½²ä¸º Web åº”ç”¨**ï¼šç”¨ Gradio æˆ– Streamlit å¿«é€Ÿæ­å»º UI

---

## å…­ã€å®Œæ•´ç¤ºä¾‹ï¼ˆç®€åŒ–ç‰ˆï¼‰

ä½ ä¹Ÿå¯ä»¥ç”¨ LangChain + OpenAI å¿«é€Ÿä½“éªŒï¼ˆéœ€ API Keyï¼‰ï¼š

```python
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI
from langchain.vectorstores import FAISS
from langchain.embeddings.openai import OpenAIEmbeddings

# å‡è®¾å·²æœ‰ vectorstore
qa = RetrievalQA.from_chain_type(
    llm=OpenAI(),
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)
print(qa.run("äººå·¥æ™ºèƒ½èµ·æºäºå“ªä¸€å¹´ï¼Ÿ"))
```