# 激活函数
激活函数：解决 “线性模型表达能力不足” 的问题，通过非线性变换，让神经网络能够学习复杂的非线性关系（比如图像中的边缘、文本中的语义关联等），否则多层线性网络等价于单层线性网络，无法拟合复杂数据。

## 1、**Sigmoid函数**
Sigmoid函数最早是在逻辑回归中提到的，它作为解决二分类的问题出场。其值域是在[0,1]之间，输出的值可以作为分类的概率。

Sigmoid函数的公式和导数如下式所示：
![image.png](https://raw.githubusercontent.com/lishiyu2006/picgo/main/cdning/202510121143969.png)

Sigmoid函数优点：  
  1、简单、非常适用分类任务；  
Sigmoid函数缺点：  
  1、反向传播训练时有梯度消失的问题；  
  2、输出值区间为(0,1)，关于0不对称；  
  3、梯度更新在不同方向走得太远，使得优化难度增大，训练耗时；

## 2、Tanh函数
Tanh函数，其输出值在区间 [-1, 1]

![image.png](https://raw.githubusercontent.com/lishiyu2006/picgo/main/cdning/202510121145630.png)

Tanh函数优点：  
1、解决了Sigmoid函数输出值非0对称的问题；  
2、训练比Sigmoid函数快，更容易收敛；  
Tanh函数缺点：  
1、反向传播训练时有梯度消失的问题；  
2、Tanh函数和Sigmoid函数非常相似；

## 3、ReLU函数

ReLU函数是目前在神经网络使用最流行的激活函数。其函数表达式和其对应的导数非常简单：
![image.png](https://raw.githubusercontent.com/lishiyu2006/picgo/main/cdning/202510121145682.png)

ReLU函数优点：  
  1、解决了梯度消失的问题；  
  2、计算更为简单，没有Sigmoid函数和Tanh函数的指数运算；  
ReLU函数缺点：  
  1、训练时可能出现神经元死亡；

## 4、Leaky ReLU函数

Leaky ReLU函数是ReLU函数的变体。其函数和对应导数的表达式为：

![image.png](https://raw.githubusercontent.com/lishiyu2006/picgo/main/cdning/202510121146344.png)

Leaky ReLU函数优点：  
  1、解决了ReLU的神经元死亡问题；  
Leaky ReLU函数缺点：  
  1、无法为正负输入值提供一致的关系预测（不同区间函数不同）；

## 5、SoftMax激活函数

SoftMax函数通常被用在多分类网络模型中，其表达式如下

![image.png](https://raw.githubusercontent.com/lishiyu2006/picgo/main/cdning/202510121148536.png)

SoftMax函数的值域是在[0,1]之间的，并且存在多个输出，例如是一个5分类的任务，那么SoftMax函数最终的输出是对应每个类别的的概率，同时这5个类别对应的概率相加最终的结果为1。

因此在**多分类任务**的场景下，神经网络的最后一层一般都是使用SoftMax函数来作为激活函数。
