# 嵌入层 Embedding层
它能处理预先定义好的、有限的类别
### 1. one-hot 编码：神经网络的 “初始输入候选”，但非核心语义表示
首先，我们有一个one-hot编码的概念。

假设，我们中文，一共只有10个字，只是假设啊，那么我们用0-9就可以表示完

比如，这十个字就是“我从哪里来，要到何处去”

其分别对应“0-9”，如下：

我 从 哪 里 来 要 到 何 处 去

0 1 2 3 4 5 6 7 8 9

那么，其实我们只用一个列表就能表示所有的对话

如：我 从 哪 里 来 要 到 何 处 去 ——>>>[0 1 2 3 4 5 6 7 8 9]

或：我 从 何 处 来 要 到 哪 里 去 ——>>>[0 1 7 8 4 5 6 2 3 9]

但是，我们看看one-hot编码方式（详见：https://blog.csdn.net/tengyuan93/article/details/78930285）

他把上面的编码方式弄成这样

我从哪里来，要到何处去
[
[1 0 0 0 0 0 0 0 0 0]
[0 1 0 0 0 0 0 0 0 0]
[0 0 1 0 0 0 0 0 0 0]
[0 0 0 1 0 0 0 0 0 0]
[0 0 0 0 1 0 0 0 0 0]
[0 0 0 0 0 1 0 0 0 0]
[0 0 0 0 0 0 1 0 0 0]
[0 0 0 0 0 0 0 1 0 0]
[0 0 0 0 0 0 0 0 1 0]
[0 0 0 0 0 0 0 0 0 1]
]

我从何处来，要到哪里去
[
[1 0 0 0 0 0 0 0 0 0]
[0 1 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 1 0 0]
[0 0 0 0 0 0 0 0 1 0]
[0 0 0 0 1 0 0 0 0 0]
[0 0 0 0 0 1 0 0 0 0]
[0 0 0 0 0 0 1 0 0 0]
[0 0 1 0 0 0 0 0 0 0]
[0 0 0 1 0 0 0 0 0 0]
[0 0 0 0 0 0 0 0 0 1]
]
那问题来了，稀疏矩阵（二维）和列表（一维）相比，有什么优势。

很明显，计算简单嘛，稀疏矩阵做矩阵计算的时候，只需要把1对应位置的数相乘求和就行，也许你心算都能算出来；而一维列表，你能很快算出来？何况这个列表还是一行，如果是100行、1000行和或1000列呢？

所以，one-hot编码的优势就体现出来了，计算方便快捷、表达能力强。

然而，缺点也随着来了。

比如：中文大大小小简体繁体常用不常用有十几万，然后一篇文章100W字，你要表示成100W X 10W的矩阵？？？

这是它最明显的缺点。过于稀疏时，过度占用资源。
这时，Embedding层横空出世。

接下来给大家看一张图

假设：我们有一个2 x 6的矩阵，然后乘上一个6 x 3的矩阵后，变成了一个2 x 3的矩阵。

先不管它什么意思，这个过程，我们把一个12个元素的矩阵变成6个元素的矩阵，直观上，大小是不是缩小了一半？

也许你已经想到了！！！对！！！不管你想的对不对，但是embedding层，在某种程度上，就是用来降维的，降维的原理就是矩阵乘法。在卷积网络中，可以理解为特殊全连接层操作，跟1x1卷积核异曲同工！！！484很神奇！！！

这就是嵌入层的一个作用——降维。

然后中间那个10W X 20的矩阵，可以理解为查询表，也可以理解为映射表，也可以理解为过度表，whatever。

接着，既然可以降维，当然也可以升维。为什么要升维？

这张图，我要你在10米开外找出五处不同！。。。What？烦请出题者走近两步，我先把我的刀拿出来，您再说一遍题目我没听清。

当然，目测这是不可能完成的。但是我让你在一米外，也许你一瞬间就发现衣服上有个心是不同的，然后再走近半米，你又发现左上角和右上角也是不同的。再走近20厘米，又发现耳朵也不同，最后，在距离屏幕10厘米的地方，终于发现第五个不同的地方在耳朵下面一点的云。

但是，其实无限靠近并不代表认知度就高了，比如，你只能距离屏幕1厘米远的地方找，找出五处不同。。。出题人你是不是脑袋被门挤了。。。

由此可见，距离的远近会影响我们的观察效果。同理也是一样的，低维的数据可能包含的特征是非常笼统的，我们需要不停地拉近拉远来改变我们的感受野，让我们对这幅图有不同的观察点，找出我们要的茬。

embedding的又一个作用体现了。对低维的数据进行升维时，可能把一些其他特征给放大了，或者把笼统的特征给分开了。同时，这个embedding是一直在学习在优化的，就使得整个拉近拉远的过程慢慢形成一个良好的观察点。

回想一下为什么CNN层数越深准确率越高，卷积层卷了又卷，池化层池了又升，升了又降，全连接层连了又连。因为我们也不知道它什么时候突然就学到了某个有用特征。但是不管怎样，学习都是好事，所以让机器多卷一卷，多连一连，反正错了多少我会用交叉熵告诉你，怎么做才是对的我会用梯度下降算法告诉你，只要给你时间，你迟早会学懂。因此，理论上，只要层数深，只要参数足够，NN能拟合任何特征。总之，它类似于虚拟出一个关系对当前数据进行映射。这个东西也许一言难尽吧，但是目前各位只需要知道它有这些功能的就行了。

### 2. 概率向量 / 稠密语义向量：神经网络的 “核心输出 / 中间表示”，体现词义关联

#### 一、原理
神经网络会将 one-hot 编码（或其他初始输入）通过**嵌入层、注意力机制、全连接层**等模块，转化为低维的 “稠密向量”（如 Word2Vec 的词向量、BERT 的 token 向量）；

![image.png](https://raw.githubusercontent.com/lishiyu2006/picgo/main/cdning/202510151558788.png)

若任务是分类（如文本分类、词性标注），最终输出层还会通过 Softmax 激活函数，将向量转为 “概率向量”（每个维度对应一个类别的概率，总和为 1）。

- **核心特性**：这类向量的 “空间距离”（如余弦相似度、欧氏距离）直接对应词义相似度 —— 例如 “猫” 和 “狗” 的向量在空间中距离近，“猫” 和 “电脑” 的向量距离远，这正是神经网络捕捉语义关联的关键。
- **举例**：用 Word2Vec 训练后，“国王” 的向量 - “男人” 的向量 + “女人” 的向量，结果会与 “女王” 的向量高度接近，体现了向量空间的语义逻辑性。
#### 二、Embedding 的应用案例
1.搜索与推荐: 把用户浏览记录转为向量，匹配相似商品

2.智能客服: 将用户问题映射到知识库答案的向量空间

3.人脸识别: 将人脸图像编码为 128 维向量(如 FaceNet)

4.病历分析: 医疗文本 Embedding 辅助诊断(如腾讯觅影)

5.合同审查: 法律条文 Embedding 快速匹配相似案例

6.以图搜图: 图片 Embedding 相似度匹配(电商找同款）

7.信贷评估: 将消费记录转为信用风险向量
![image.png](https://raw.githubusercontent.com/lishiyu2006/picgo/main/cdning/202510151605625.png)
#### 三、静态 vs 动态 Embedding(传统身份证 vs 变色龙身份证)
##### 1.静态 Embedding(如 Word2Vec)

特点: 每个词只有唯一固定的向量，无法区分多义词

##### 2.BERT 的动态 Embedding(上下文敏感)

特点: 同一个词在不同语境中向量实时变化

 水果场景: "苹果"→ [0.2, -1.5,3.8, 0.9]

科技场景: "苹果"→> [1.7,0.3, -2.1,4.2]

效果: 模型能感知"苹果"在句子中的真实含义

注① Word2Vec: Google 在 2013年提出的模型，用于生成词向量

注② BERT: Google 在 2018年提出的模型,首次实现了真正意义上的上下文感知的词向量表示
#### 四、大模型生成embedding怎么做?

大模型embedding VS 小模型embedding

##### 1.简介:

[BERT](../4.预训练模型/预训练语言模型%201/BERT.md): 更适合需要双向上下文理解的任务，通过微调获得高性能

[LLM](../4.预训练模型/预训练语言模型%201/LLM.md): 在零样本场景和生成任务中表现优越，但需设计合理的Embedding提取策略
