# 基本知识：
深度学习领域的核心概念包含关系可概括为：框架 ⊃ 模型架构 ⊃ 层，三者是“工具-成品-组件”的层级关系。

## 1. 最外层：深度学习框架（如PyTorch、TensorFlow）

框架是最大的“容器”，是一套完整的开发工具集。它封装了所有底层技术（如矩阵运算、梯度优化），并提供了构建模型所需的“基础零件库”——即各种现成的“层”（卷积层、全连接层等）。
你可以把它理解为“乐高套装”，套装里的积木就是“层”。

## 2. 中间层：模型架构（如CNN、Transformer、GAN、RNN/LSTM/GRN、GNN）

模型架构是用框架里的“层”搭建出的“成品设计图”。它规定了“层”的组合方式：用哪些层、层的顺序、层间如何连接。
比如“CNN架构”明确要求用“卷积层+池化层”交替堆叠，再接“全连接层”；“Transformer架构”则以“多头自注意力层+前馈神经网络层”为核心组件。
这相当于用乐高积木拼搭“城堡”（CNN）或“汽车”（Transformer）的“说明书”。

## 3. 最内层：层（如卷积层、自注意力层、全连接层）

层是构成模型架构的“最小功能单元”，是实现特定计算的模块。不同模型架构会选用不同的层组合：
• CNN的核心层是卷积层(缩写也是CNN)、池化层；
• Transformer的核心层是多头自注意力层、前馈神经网络层；
• 所有模型几乎都会用到激活层、归一化层等基础层。
这就像乐高积木里的“齿轮块”“平板块”，每种块有特定功能，按需组合成不同成品。

除了卷积层，深度学习中还有多种常用的“层”，核心可分为以下几类：
1.基础运算层
◦ 全连接层：Dense Layer（DL）
◦ 激活函数层：Activation Layer（AL）
◦ 归一化层：Normalization Layer（NL），细分如批归一化（BN）、层归一化（LN）
2.序列处理相关层
◦ 循环层：Recurrent Layer（RL），细分如循环神经网络（RNN）、长短期记忆网络（LSTM）、门控循环单元（GRU）
◦ 注意力层：Attention Layer（AttnL），细分如多头自注意力层（MHSA）
3.特殊功能层
◦ 池化层：Pooling Layer（PL），细分如最大池化（MaxPool）、平均池化（AvgPool）
◦ 嵌入层：Embedding Layer（EmbL）
◦ Dropout层：Dropout Layer（DL，易与全连接层混淆，通常直接称Dropout）
