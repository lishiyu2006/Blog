# RAG
RAGï¼ˆRetrieval-Augmented Generationï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ˜¯ä¸€ç§ç»“åˆä¿¡æ¯æ£€ç´¢ä¸è¯­è¨€ç”Ÿæˆçš„æŠ€æœ¯ï¼Œå¸¸ç”¨äºæå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç‰¹å®šé¢†åŸŸæˆ–çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§ä¸ç›¸å…³æ€§ã€‚ä¸‹é¢æˆ‘å°†å¸¦ä½ ä¸€æ­¥æ­¥å®æ“ä¸€ä¸ªç®€å•çš„ RAG ç³»ç»Ÿã€‚
## ğŸ§° ä¸€ã€RAG åŸºæœ¬åŸç†

RAG = **æ£€ç´¢å™¨ï¼ˆRetrieverï¼‰** + **ç”Ÿæˆå™¨ï¼ˆGeneratorï¼‰**

1. **ç”¨æˆ·æé—®**Â â†’
2. **æ£€ç´¢å™¨**Â ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼ˆå¦‚ PDFã€ç½‘é¡µã€æ•°æ®åº“ç­‰ï¼‰â†’
3. **å°†é—®é¢˜ + æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡**Â è¾“å…¥ç»™ LLM â†’
4. **LLM ç”Ÿæˆç­”æ¡ˆ**ï¼ˆåŸºäºæ£€ç´¢å†…å®¹ï¼Œé¿å…â€œå¹»è§‰â€ï¼‰

---

## ğŸ› ï¸ äºŒã€å®æ“ç¯å¢ƒå‡†å¤‡

æˆ‘ä»¬å°†ä½¿ç”¨ Python + å¼€æºå·¥å…·æ­å»ºä¸€ä¸ªæœ¬åœ° RAG ç³»ç»Ÿã€‚

### æ‰€éœ€åº“ï¼š

```bash
pip install langchain
pip install faiss-cpu  # å‘é‡æ•°æ®åº“ï¼ˆä¹Ÿå¯ç”¨ chromaã€weaviate ç­‰ï¼‰
pip install sentence-transformers  # ç”¨äºåµŒå…¥ï¼ˆEmbeddingï¼‰
pip install transformers  # å¯é€‰ï¼Œç”¨äºæœ¬åœ° LLM
pip install pypdf  # å¦‚æœè¦è¯»å– PDF
```
> ğŸ’¡ ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ OpenAI API ä½œä¸º LLMï¼Œä½†è¿™é‡Œæˆ‘ä»¬å°½é‡ç”¨å¼€æºæ–¹æ¡ˆã€‚

---

## ğŸ“š ä¸‰ã€å‡†å¤‡çŸ¥è¯†åº“ï¼ˆç¤ºä¾‹ï¼‰

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªæœ¬åœ°çŸ¥è¯†åº“ï¼š`data/` ç›®å½•ä¸‹æœ‰å‡ ä¸ª `.txt` æ–‡ä»¶ï¼Œå†…å®¹æ˜¯å…³äºâ€œäººå·¥æ™ºèƒ½å‘å±•å²â€çš„ç‰‡æ®µã€‚

ä¾‹å¦‚ï¼š`ai_history.txt`
```
äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰èµ·æºäº20ä¸–çºª50å¹´ä»£ã€‚1956å¹´è¾¾ç‰¹èŒ…æ–¯ä¼šè®®è¢«è®¤ä¸ºæ˜¯AIçš„è¯ç”Ÿæ ‡å¿—ã€‚
...
```
## ğŸ” å››ã€æ„å»º RAG ç³»ç»Ÿï¼ˆä»£ç ï¼‰

### æ­¥éª¤ 1ï¼šåŠ è½½æ–‡æ¡£
```python
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# åŠ è½½æ–‡æ¡£
loader = TextLoader("data/ai_history.txt", encoding="utf-8")
documents = loader.load()

# åˆ†å—ï¼ˆchunkï¼‰
text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
chunks = text_splitter.split_documents(documents)
```
### æ­¥éª¤ 2ï¼šåˆ›å»ºå‘é‡æ•°æ®åº“ï¼ˆä½¿ç”¨ FAISSï¼‰
```python
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

# ä½¿ç”¨å¼€æºåµŒå…¥æ¨¡å‹ï¼ˆå¦‚ all-MiniLM-L6-v2ï¼‰
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# æ„å»ºå‘é‡åº“
vectorstore = FAISS.from_documents(chunks, embeddings)
```
### æ­¥éª¤ 3ï¼šè®¾ç½®æ£€ç´¢å™¨
```python
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})  # è¿”å›æœ€ç›¸å…³çš„3ä¸ªç‰‡
```
### æ­¥éª¤ 4ï¼šé€‰æ‹© LLMï¼ˆè¿™é‡Œç”¨ HuggingFace çš„æœ¬åœ°æ¨¡å‹ï¼‰
```python

from langchain.llms import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# ä½¿ç”¨è¾ƒå°çš„å¼€æºæ¨¡å‹ï¼ˆå¦‚ google/flan-t5-baseï¼‰
model_name = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

pipe = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer,
    max_length=512,
    temperature=0.0,
)

llm = HuggingFacePipeline(pipeline=pipe)
```
> âš ï¸ æ³¨æ„ï¼š`flan-t5-base` æ˜¯ encoder-decoder æ¨¡å‹ï¼Œé€‚åˆé—®ç­”ï¼›è‹¥ç”¨ LLaMA ç­‰ decoder-only æ¨¡å‹ï¼Œéœ€è°ƒæ•´ pipelineã€‚

### æ­¥éª¤ 5ï¼šæ„å»º RAG é“¾


```python
from langchain.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",  # å°†æ‰€æœ‰æ£€ç´¢ç»“æœæ‹¼æ¥åè¾“å…¥ LLM
    retriever=retriever,
    return_source_documents=True
)
```
### æ­¥éª¤ 6ï¼šæé—®å¹¶æŸ¥çœ‹ç»“æœ
```python
query = "äººå·¥æ™ºèƒ½æ˜¯åœ¨å“ªä¸€å¹´æ­£å¼æå‡ºçš„ï¼Ÿ"
result = qa_chain({"query": query})

print("å›ç­”:", result["result"])
print("å‚è€ƒæ¥æº:")
for doc in result["source_documents"]:
    print("-", doc.page_content[:100] + "...")
```
## ğŸŒ äº”ã€è¿›é˜¶å»ºè®®

1. **ä½¿ç”¨æ›´å¼ºå¤§çš„ LLM**ï¼šå¦‚ LLaMA-2ã€ChatGLMã€Qwenï¼ˆéœ€ GPUï¼‰
2. **ä½¿ç”¨ Chroma æˆ– Weaviate**Â æ›¿ä»£ FAISSï¼Œæ”¯æŒæŒä¹…åŒ–
3. **åŠ å…¥ reranker**ï¼ˆå¦‚ Cohere Rerank æˆ– BAAI/bge-rerankerï¼‰æå‡æ£€ç´¢è´¨é‡
4. **éƒ¨ç½²ä¸º Web åº”ç”¨**ï¼šç”¨ Gradio æˆ– Streamlit å¿«é€Ÿæ­å»º UI
---

## ğŸ§ª å…­ã€å®Œæ•´ç¤ºä¾‹ï¼ˆç®€åŒ–ç‰ˆï¼‰

ä½ ä¹Ÿå¯ä»¥ç”¨ LangChain + OpenAI å¿«é€Ÿä½“éªŒï¼ˆéœ€ API Keyï¼‰ï¼š

```python
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI
from langchain.vectorstores import FAISS
from langchain.embeddings.openai import OpenAIEmbeddings

# å‡è®¾å·²æœ‰ vectorstore
qa = RetrievalQA.from_chain_type(
    llm=OpenAI(),
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)
print(qa.run("äººå·¥æ™ºèƒ½èµ·æºäºå“ªä¸€å¹´ï¼Ÿ"))
```