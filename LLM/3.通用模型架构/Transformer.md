### 一、Transformer 的核心设计：解决 “长序列依赖” 与 “并行计算”

传统 RNN 处理序列时需 “逐元素迭代”（如从文本第一个词算到最后一个），不仅效率低，还难以捕捉长距离语义关联（如 “猫” 和隔 10 个词的 “抓老鼠”）。Transformer 用**自注意力机制**打破这一限制，同时通过 “并行计算” 大幅提升训练效率，其核心设计可拆解为 “整体结构” 和 “关键组件” 两部分：

#### 1. 整体结构：编码器（Encoder）+ 解码器（Decoder）

Transformer 本质是 “编码器 - 解码器架构”，但不同任务可灵活选择使用部分结构（如文本理解用编码器，文本生成用解码器）：
```plaintext
[输入序列] → 词嵌入+位置编码 → [编码器层×N] → [编码器输出]
											  ↓
[目标序列] → 词嵌入+位置编码 → [解码器层×N] → [输出序列（如翻译结果）]
```
